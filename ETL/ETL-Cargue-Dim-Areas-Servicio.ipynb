{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcY9y56n86vn"
   },
   "source": [
    "# ETL: Carga Areas de Servicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:39:56.271810Z",
     "start_time": "2024-11-16T13:39:56.268816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from pyspark.sql.types import IntegerType, StringType, DateType, LongType\n",
    "from pyspark.sql import functions as f, SparkSession, types as t\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import col, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:39:59.561504Z",
     "start_time": "2024-11-16T13:39:59.556957Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySQLConnector:\n",
    "    def __init__(self, spark: SparkSession, connection_properties: dict, url: str):\n",
    "        self.spark = spark\n",
    "        self.properties = connection_properties\n",
    "        self.url = url\n",
    "\n",
    "    def get_dataframe(self, sql_query: str):        \n",
    "        df = self.spark.read.jdbc(\n",
    "            url=self.url,\n",
    "            table=sql_query,\n",
    "            properties=self.properties\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def save_db(self, df, tabla):\n",
    "        df.write.jdbc(\n",
    "            url=self.url,\n",
    "            table=tabla,\n",
    "            mode='append',\n",
    "            properties=self.properties\n",
    "        )\n",
    "        \n",
    "def create_spark_session(path_jar_driver):    \n",
    "    conf = SparkConf().set('spark.driver.extraClassPath', path_jar_driver)\n",
    "    spark_context = SparkContext(conf=conf)\n",
    "    sql_context = SQLContext(spark_context)\n",
    "    return sql_context.sparkSession    \n",
    "\n",
    "def get_dataframe_from_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:40:29.103266Z",
     "start_time": "2024-11-16T13:40:29.093571Z"
    },
    "id": "os1iJYmu86vt"
   },
   "outputs": [],
   "source": [
    "# LLENAR CON EL USUARIO DE CADA UNO\n",
    "db_user = 'Estudiante_8_202415'\n",
    "db_psswd = 'Estudiante_200723002'\n",
    "\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_psswd,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source_db_string_connection = 'jdbc:mysql://157.253.236.120:8080/RaSaTransaccional_ETL'\n",
    "destination_db_string_connection = f'jdbc:mysql://157.253.236.120:8080/{db_user}'\n",
    "\n",
    "# Driver de conexion\n",
    "# LINUX\n",
    "#path_jar_driver = '/opt/mysql/lib/mysql-connector-java-8.0.28.jar'\n",
    "# WINDOWS\n",
    "path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:40:36.484852Z",
     "start_time": "2024-11-16T13:40:33.016452Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\estudiante\\AppData\\Local\\Temp\\ipykernel_12468\\3043745556.py:25 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [172]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_jar_driver\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [171]\u001b[0m, in \u001b[0;36mcreate_spark_session\u001b[1;34m(path_jar_driver)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_spark_session\u001b[39m(path_jar_driver):    \n\u001b[0;32m     24\u001b[0m     conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.driver.extraClassPath\u001b[39m\u001b[38;5;124m'\u001b[39m, path_jar_driver)\n\u001b[1;32m---> 25\u001b[0m     spark_context \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     sql_context \u001b[38;5;241m=\u001b[39m SQLContext(spark_context)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sql_context\u001b[38;5;241m.\u001b[39msparkSession\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    454\u001b[0m             currentAppName,\n\u001b[0;32m    455\u001b[0m             currentMaster,\n\u001b[0;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    459\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\estudiante\\AppData\\Local\\Temp\\ipykernel_12468\\3043745556.py:25 "
     ]
    }
   ],
   "source": [
    "spark = create_spark_session(path_jar_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:40:40.941673Z",
     "start_time": "2024-11-16T13:40:40.931528Z"
    }
   },
   "outputs": [],
   "source": [
    "conn_orig = MySQLConnector(spark=spark, connection_properties=connection_properties, url=source_db_string_connection)\n",
    "conn_dest = MySQLConnector(spark=spark, connection_properties=connection_properties, url=destination_db_string_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqQVz5s686vq"
   },
   "source": [
    "## Proceso de ETL para una dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpOSEfhX86vs"
   },
   "source": [
    "![Modelo Movimientos](./images/AreasDeServicio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cargar FuenteAreasDeServicio_ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros de FuenteAreasDeServicio_ETL:\n",
      "+------------------+----------------------------------------------+-------------+-----------------+----------+------------+-----+--------+-----+\n",
      "|IdAreaDeServicio_T|NombreAreaDeServicio                          |IdGeografia_T|Condado          |Estado    |PoblacionAct|Area |Densidad|Fecha|\n",
      "+------------------+----------------------------------------------+-------------+-----------------+----------+------------+-----+--------+-----+\n",
      "|100622017         |New Jersey - Medical91661NJ2340003-0520174859 |34005        |Burlington County|New Jersey|464269      |805.0|577.0   |2017 |\n",
      "|100722019         |New Jersey  - Medical91661NJ2340003-0520194597|34023        |Middlesex County |New Jersey|860807      |311.0|2768.0  |2019 |\n",
      "|100922020         |New Jersey - Medical91661NJ2340003-0520204858 |34019        |Hunterdon County |New Jersey|129924      |430.0|302.0   |2020 |\n",
      "|101012018         |New Jersey - Medical91661NJ2340003-0520184611 |34031        |Passaic County   |New Jersey|518117      |185.0|2801.0  |2018 |\n",
      "|101062020         |New Jersey - Medical91661NJ2340003-0520205037 |34031        |Passaic County   |New Jersey|518117      |185.0|2801.0  |2020 |\n",
      "+------------------+----------------------------------------------+-------------+-----------------+----------+------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracción de todos los campos de FuenteAreasDeServicio_ETL\n",
    "sql_areas_servicio_all_fields = '''\n",
    "(SELECT *\n",
    " FROM RaSaTransaccional_ETL.FuenteAreasDeServicio_ETL) AS AreasDeServicio\n",
    "'''\n",
    "\n",
    "try:\n",
    "    df_areas_servicio = conn_orig.get_dataframe(sql_areas_servicio_all_fields)\n",
    "    print(\"Primeros 5 registros de FuenteAreasDeServicio_ETL:\")\n",
    "    df_areas_servicio.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al extraer datos de FuenteAreasDeServicio_ETL:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Areas de Servicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de datos de AreasDeServicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros de AreasDeServicio:\n",
      "+--------------------+------------------+----------------------------------------------+------------+\n",
      "|IdAreaDeServicio_DWH|IdAreaDeServicio_T|Nombre                                        |AnnoCreacion|\n",
      "+--------------------+------------------+----------------------------------------------+------------+\n",
      "|100622017           |100622017         |New Jersey - Medical91661NJ2340003-0520174859 |2017        |\n",
      "|100722019           |100722019         |New Jersey  - Medical91661NJ2340003-0520194597|2019        |\n",
      "|100922020           |100922020         |New Jersey - Medical91661NJ2340003-0520204858 |2020        |\n",
      "|101012018           |101012018         |New Jersey - Medical91661NJ2340003-0520184611 |2018        |\n",
      "|101062020           |101062020         |New Jersey - Medical91661NJ2340003-0520205037 |2020        |\n",
      "+--------------------+------------------+----------------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracción de datos de AreasDeServicio\n",
    "sql_areas_servicio = '''\n",
    "(SELECT DISTINCT \n",
    "    IdAreaDeServicio_T AS IdAreaDeServicio_DWH, \n",
    "    IdAreaDeServicio_T, \n",
    "    NombreAreaDeServicio AS Nombre, \n",
    "    Fecha AS AnnoCreacion \n",
    " FROM RaSaTransaccional_ETL.FuenteAreasDeServicio_ETL) AS AreasDeServicio\n",
    "'''\n",
    "\n",
    "try:\n",
    "    df_areas_servicio = conn_orig.get_dataframe(sql_areas_servicio)\n",
    "    print(\"Primeros 5 registros de AreasDeServicio:\")\n",
    "    df_areas_servicio.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error durante la extracción de datos de AreasDeServicio:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Crear un identificador único y consecutivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------+\n",
      "|IdAreaDeServicio_DWH|IdAreaDeServicio_T|              Nombre|AnnoCreacion|\n",
      "+--------------------+------------------+--------------------+------------+\n",
      "|                   1|         100622017|New Jersey - Medi...|        2017|\n",
      "|                   2|         100722019|New Jersey  - Med...|        2019|\n",
      "|                   3|         100922020|New Jersey - Medi...|        2020|\n",
      "|                   4|         101012018|New Jersey - Medi...|        2018|\n",
      "|                   5|         101062020|New Jersey - Medi...|        2020|\n",
      "+--------------------+------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_areas_servicio = df_areas_servicio.withColumn(\"IdAreaDeServicio_DWH\", monotonically_increasing_id() + 1)\n",
    "\n",
    "df_areas_servicio.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validación de duplicados para IdAreaDeServicio_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron duplicados en IdAreaDeServicio_T\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df_areas_servicio.groupBy(\"IdAreaDeServicio_T\").count().filter(col(\"count\") > 1).count()\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Se encontraron {duplicate_count} duplicados en IdAreaDeServicio_T\")\n",
    "else:\n",
    "    print(\"No se encontraron duplicados en IdAreaDeServicio_T\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modificar Nombre para quitar espacios etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------+\n",
      "|IdAreaDeServicio_DWH|IdAreaDeServicio_T|              Nombre|AnnoCreacion|\n",
      "+--------------------+------------------+--------------------+------------+\n",
      "|                   1|         100622017|New Jersey - Medi...|        2017|\n",
      "|                   2|         100722019|New Jersey - Medi...|        2019|\n",
      "|                   3|         100922020|New Jersey - Medi...|        2020|\n",
      "|                   4|         101012018|New Jersey - Medi...|        2018|\n",
      "|                   5|         101062020|New Jersey - Medi...|        2020|\n",
      "+--------------------+------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, regexp_replace\n",
    "\n",
    "df_areas_servicio = df_areas_servicio.withColumn(\n",
    "    \"Nombre\",\n",
    "    trim(regexp_replace(col(\"Nombre\"), \" {2,}\", \" \"))\n",
    ")\n",
    "\n",
    "df_areas_servicio.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Revisar el tipo de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IdAreaDeServicio_DWH: long (nullable = false)\n",
      " |-- IdAreaDeServicio_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- AnnoCreacion: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_areas_servicio.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar los datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o611.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 7) (MISW--09682.sis.virtual.uniandes.edu.co executor driver): java.sql.BatchUpdateException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\r\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 19 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [180]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carga de los datos transformados\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mconn_dest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_areas_servicio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEstudiante_8_202415.Rs_AreasDeServicio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatos cargados correctamente en la tabla \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRs_AreasDeServicio\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [171]\u001b[0m, in \u001b[0;36mMySQLConnector.save_db\u001b[1;34m(self, df, tabla)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_db\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, tabla):\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtabla\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o611.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 7) (MISW--09682.sis.virtual.uniandes.edu.co executor driver): java.sql.BatchUpdateException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\r\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Duplicate entry '1000' for key 'Rs_AreasDeServicio.PRIMARY'\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 19 more\r\n"
     ]
    }
   ],
   "source": [
    "# Carga de los datos transformados\n",
    "conn_dest.save_db(df_areas_servicio, 'Estudiante_8_202415.Rs_AreasDeServicio')\n",
    "\n",
    "print(\"Datos cargados correctamente en la tabla 'Rs_AreasDeServicio'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Geografía "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de datos de Geografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros de Geografia:\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado    |Condado          |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|34005          |34005        |New Jersey|Burlington County|805.0  |577.0      |464269      |\n",
      "|34023          |34023        |New Jersey|Middlesex County |311.0  |2768.0     |860807      |\n",
      "|34019          |34019        |New Jersey|Hunterdon County |430.0  |302.0      |129924      |\n",
      "|34031          |34031        |New Jersey|Passaic County   |185.0  |2801.0     |518117      |\n",
      "|34037          |34037        |New Jersey|Sussex County    |521.0  |279.0      |145543      |\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracción de datos de Geografia\n",
    "sql_geografia = '''\n",
    "(SELECT DISTINCT \n",
    "    IdGeografia_T AS IdGeografia_DWH, \n",
    "    IdGeografia_T, \n",
    "    Estado, \n",
    "    Condado, \n",
    "    Area AS AreaAct, \n",
    "    Densidad AS DensidadAct, \n",
    "    PoblacionAct \n",
    " FROM RaSaTransaccional_ETL.FuenteAreasDeServicio_ETL) AS Geografia\n",
    "'''\n",
    "\n",
    "try:\n",
    "    df_geografia = conn_orig.get_dataframe(sql_geografia)\n",
    "    print(\"Primeros 5 registros de Geografia:\")\n",
    "    df_geografia.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error durante la extracción de datos de Geografia:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Crear un identificador único y consecutivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado    |Condado          |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|1              |34005        |New Jersey|Burlington County|805.0  |577.0      |464269      |\n",
      "|2              |34023        |New Jersey|Middlesex County |311.0  |2768.0     |860807      |\n",
      "|3              |34019        |New Jersey|Hunterdon County |430.0  |302.0      |129924      |\n",
      "|4              |34031        |New Jersey|Passaic County   |185.0  |2801.0     |518117      |\n",
      "|5              |34037        |New Jersey|Sussex County    |521.0  |279.0      |145543      |\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Crear identificador único y consecutivo\n",
    "df_geografia = df_geografia.withColumn(\"IdGeografia_DWH\", monotonically_increasing_id() + 1)\n",
    "\n",
    "# Validar los cambios\n",
    "df_geografia.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eliminar áreas negativas -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado    |Condado          |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|1              |34005        |New Jersey|Burlington County|805.0  |577.0      |464269      |\n",
      "|2              |34023        |New Jersey|Middlesex County |311.0  |2768.0     |860807      |\n",
      "|3              |34019        |New Jersey|Hunterdon County |430.0  |302.0      |129924      |\n",
      "|4              |34031        |New Jersey|Passaic County   |185.0  |2801.0     |518117      |\n",
      "|5              |34037        |New Jersey|Sussex County    |521.0  |279.0      |145543      |\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Corregir los valores negativos en la columna AreaAct\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_geografia = df_geografia.withColumn(\n",
    "    \"AreaAct\",\n",
    "    when(col(\"AreaAct\") < 0, col(\"AreaAct\") * -1).otherwise(col(\"AreaAct\"))\n",
    ")\n",
    "\n",
    "# Mostrar los primeros 5 registros transformados\n",
    "df_geografia.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron áreas negativas.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filtrar las filas con áreas negativas\n",
    "areas_negativas = df_geografia.filter(col(\"AreaAct\") < 0)\n",
    "\n",
    "# Contar el número de filas con áreas negativas\n",
    "num_areas_negativas = areas_negativas.count()\n",
    "\n",
    "# Imprimir resultados\n",
    "if num_areas_negativas > 0:\n",
    "    print(f\"Se encontraron {num_areas_negativas} áreas negativas:\")\n",
    "    areas_negativas.show()\n",
    "else:\n",
    "    print(\"No se encontraron áreas negativas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Errores en la población "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-----------+--------------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado     |Condado             |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+-----------+--------------------+-------+-----------+------------+\n",
      "|2647           |51109        |Virginia   |Louisa County       |498.0  |78.0       |38848.0001  |\n",
      "|2648           |30045        |Montana    |Judith Basin County |1870.0 |1.0        |2044.0001   |\n",
      "|2649           |28111        |Mississippi|Perry County        |647.0  |18.0       |11571.0001  |\n",
      "|2650           |17149        |Illinois   |Pike County         |831.0  |18.0       |14618.0001  |\n",
      "|2651           |18099        |Indiana    |Marshall County     |444.0  |104.0      |46121.0001  |\n",
      "|2652           |12095        |Florida    |Orange County       |908.0  |1567.0     |1422746.0001|\n",
      "|2653           |48273        |Texas      |Kleberg County      |871.0  |35.0       |30635.0001  |\n",
      "|2654           |23031        |Maine      |York County         |1271.0 |169.0      |214591.0001 |\n",
      "|2655           |48047        |Texas      |Brooks County       |943.0  |7.0        |6994.0001   |\n",
      "|2656           |12035        |Florida    |Flagler County      |485.0  |249.0      |120932.0001 |\n",
      "|2657           |13109        |Georgia    |Evans County        |185.0  |58.0       |10672.0001  |\n",
      "|2658           |48101        |Texas      |Cottle County       |901.0  |2.0        |1381.0001   |\n",
      "|2659           |55077        |Wisconsin  |Marquette County    |455.0  |35.0       |15792.0001  |\n",
      "|2660           |17139        |Illinois   |Moultrie County     |335.0  |43.0       |14510.0001  |\n",
      "|2661           |51093        |Virginia   |Isle of Wight County|316.0  |124.0      |39278.0001  |\n",
      "|2662           |35001        |New Mexico |Bernalillo County   |1166.0 |578.0      |674393.0001 |\n",
      "|2663           |40059        |Oklahoma   |Harper County       |1039.0 |3.0        |3180.0001   |\n",
      "|2664           |18119        |Indiana    |Owen County         |385.0  |56.0       |21446.0001  |\n",
      "|2665           |19115        |Iowa       |Louisa County       |402.0  |27.0       |10749.0001  |\n",
      "|2666           |29510        |Missouri   |Saint Louis City    |61.0   |4808.0     |293310.0001 |\n",
      "+---------------+-------------+-----------+--------------------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Se encontraron 2452 registros con PoblacionAct terminando en '0001'.\n"
     ]
    }
   ],
   "source": [
    "# Filtrar registros donde PoblacionAct termine en \"0001\"\n",
    "df_invalid_poblacion = df_geografia.filter(col(\"PoblacionAct\").like(\"%0001\"))\n",
    "\n",
    "# Mostrar los registros con PoblacionAct inválido\n",
    "df_invalid_poblacion.show(truncate=False)\n",
    "\n",
    "# Contar los registros con valores inválidos\n",
    "count_invalid = df_invalid_poblacion.count()\n",
    "if count_invalid > 0:\n",
    "    print(f\"Se encontraron {count_invalid} registros con PoblacionAct terminando en '0001'.\")\n",
    "else:\n",
    "    print(\"No se encontraron registros con PoblacionAct terminando en '0001'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado    |Condado          |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "|1              |34005        |New Jersey|Burlington County|805.0  |577.0      |464269      |\n",
      "|2              |34023        |New Jersey|Middlesex County |311.0  |2768.0     |860807      |\n",
      "|3              |34019        |New Jersey|Hunterdon County |430.0  |302.0      |129924      |\n",
      "|4              |34031        |New Jersey|Passaic County   |185.0  |2801.0     |518117      |\n",
      "|5              |34037        |New Jersey|Sussex County    |521.0  |279.0      |145543      |\n",
      "|6              |34035        |New Jersey|Somerset County  |305.0  |1133.0     |345647      |\n",
      "|7              |21041        |Kentucky  |Carroll County   |130.0  |84.0       |10863       |\n",
      "|8              |12031        |Florida   |Duval County     |774.0  |1292.0     |999935      |\n",
      "|9              |34027        |New Jersey|Morris County    |469.0  |1090.0     |510981      |\n",
      "|10             |34041        |New Jersey|Warren County    |358.0  |309.0      |110731      |\n",
      "|11             |34025        |New Jersey|Monmouth County  |472.0  |1367.0     |645354      |\n",
      "|12             |34017        |New Jersey|Hudson County    |47.0   |14946.0    |702463      |\n",
      "|13             |34003        |New Jersey|Bergen County    |234.0  |4076.0     |953819      |\n",
      "|14             |21033        |Kentucky  |Caldwell County  |347.0  |36.0       |12624       |\n",
      "|15             |12107        |Florida   |Putnam County    |722.0  |103.0      |74167       |\n",
      "|16             |34029        |New Jersey|Ocean County     |636.0  |1020.0     |648998      |\n",
      "|17             |34021        |New Jersey|Mercer County    |226.0  |1708.0     |385898      |\n",
      "|18             |12059        |Florida   |Holmes County    |482.0  |41.0       |19784       |\n",
      "|19             |12035        |Florida   |Flagler County   |485.0  |249.0      |120932      |\n",
      "|20             |12101        |Florida   |Pasco County     |745.0  |784.0      |584067      |\n",
      "+---------------+-------------+----------+-----------------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, floor\n",
    "\n",
    "# Corregir los valores de PoblacionAct con \".0001\" al eliminar los últimos cuatro dígitos\n",
    "df_geografia = df_geografia.withColumn(\n",
    "    \"PoblacionAct\",\n",
    "    when(col(\"PoblacionAct\").like(\"%.0001\"), floor(col(\"PoblacionAct\") / 10000))\n",
    "    .otherwise(col(\"PoblacionAct\"))\n",
    ")\n",
    "\n",
    "# Validar la transformación\n",
    "df_geografia.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Validar si existen duplicados para IdGeografia_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron valores nulos en la columna IdGeografia_T.\n",
      "No se encontraron valores nulos en la columna Estado.\n",
      "No se encontraron valores nulos en la columna Condado.\n",
      "Se encontraron 59 valores nulos en la columna AreaAct.\n",
      "Se encontraron 59 valores nulos en la columna DensidadAct.\n",
      "No se encontraron valores nulos en la columna PoblacionAct.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si existen valores nulos en las columnas relevantes\n",
    "columns_to_check = [\"IdGeografia_T\", \"Estado\", \"Condado\", \"AreaAct\", \"DensidadAct\", \"PoblacionAct\"]\n",
    "\n",
    "for col_name in columns_to_check:\n",
    "    null_count = df_geografia.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"Se encontraron {null_count} valores nulos en la columna {col_name}.\")\n",
    "    else:\n",
    "        print(f\"No se encontraron valores nulos en la columna {col_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------+---------------------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado  |Condado              |AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+--------+---------------------+-------+-----------+------------+\n",
      "|71             |51510        |Virginia|Alexandria City      |NULL   |NULL       |158309      |\n",
      "|216            |51650        |Virginia|Hampton City         |NULL   |NULL       |135169      |\n",
      "|219            |51735        |Virginia|Poquoson City        |NULL   |NULL       |12121       |\n",
      "|220            |51830        |Virginia|Williamsburg City    |NULL   |NULL       |15034       |\n",
      "|221            |51710        |Virginia|Norfolk City         |NULL   |NULL       |244300      |\n",
      "|222            |51700        |Virginia|Newport News City    |NULL   |NULL       |179582      |\n",
      "|452            |51670        |Virginia|Hopewell City        |NULL   |NULL       |22500       |\n",
      "|453            |51570        |Virginia|Colonial Heights City|NULL   |NULL       |17283       |\n",
      "|454            |51730        |Virginia|Petersburg City      |NULL   |NULL       |30791       |\n",
      "|459            |51760        |Virginia|Richmond City        |NULL   |NULL       |229233      |\n",
      "|863            |51810        |Virginia|Virginia Beach City  |NULL   |NULL       |450882      |\n",
      "|864            |51550        |Virginia|Chesapeake City      |NULL   |NULL       |242647      |\n",
      "|1023           |51610        |Virginia|Falls Church City    |NULL   |NULL       |14309       |\n",
      "|1212           |51685        |Virginia|Manassas Park City   |NULL   |NULL       |17548       |\n",
      "|1213           |51600        |Virginia|Fairfax City         |NULL   |NULL       |23312       |\n",
      "|1247           |51740        |Virginia|Portsmouth City      |NULL   |NULL       |94961       |\n",
      "|1699           |51620        |Virginia|Franklin City        |NULL   |NULL       |8015        |\n",
      "|1823           |51630        |Virginia|Fredericksburg City  |NULL   |NULL       |29059       |\n",
      "|1897           |51678        |Virginia|Lexington City       |NULL   |NULL       |7205        |\n",
      "|1985           |51683        |Virginia|Manassas City        |NULL   |NULL       |41038       |\n",
      "+---------------+-------------+--------+---------------------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total de registros con valores nulos en AreaAct o DensidadAct: 59\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filtrar registros donde las columnas AreaAct o DensidadAct son nulas\n",
    "nulos_area_densidad = df_geografia.filter(col(\"AreaAct\").isNull() | col(\"DensidadAct\").isNull())\n",
    "\n",
    "# Contar y mostrar registros con valores nulos\n",
    "nulos_area_densidad.show(truncate=False)\n",
    "print(f\"Total de registros con valores nulos en AreaAct o DensidadAct: {nulos_area_densidad.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------+-----------------+-------+-----------+------------+\n",
      "|Estado    |IdGeografia_DWH|IdGeografia_T|Condado          |AreaAct|DensidadAct|PoblacionAct|\n",
      "+----------+---------------+-------------+-----------------+-------+-----------+------------+\n",
      "|New Jersey|1              |34005        |Burlington County|805.0  |577.0      |464269      |\n",
      "|New Jersey|2              |34023        |Middlesex County |311.0  |2768.0     |860807      |\n",
      "|New Jersey|3              |34019        |Hunterdon County |430.0  |302.0      |129924      |\n",
      "|New Jersey|4              |34031        |Passaic County   |185.0  |2801.0     |518117      |\n",
      "|New Jersey|5              |34037        |Sussex County    |521.0  |279.0      |145543      |\n",
      "+----------+---------------+-------------+-----------------+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, col, when\n",
    "\n",
    "# Calcular los promedios por Estado para AreaAct y DensidadAct\n",
    "promedios_por_estado = df_geografia.groupBy(\"Estado\").agg(\n",
    "    mean(\"AreaAct\").alias(\"PromedioAreaEstado\"),\n",
    "    mean(\"DensidadAct\").alias(\"PromedioDensidadEstado\")\n",
    ")\n",
    "\n",
    "# Unir los promedios calculados al DataFrame original\n",
    "df_geografia = df_geografia.join(promedios_por_estado, on=\"Estado\", how=\"left\")\n",
    "\n",
    "# Rellenar los valores nulos en AreaAct y DensidadAct usando los promedios por Estado\n",
    "df_geografia = df_geografia.withColumn(\n",
    "    \"AreaAct\",\n",
    "    when(col(\"AreaAct\").isNull(), col(\"PromedioAreaEstado\")).otherwise(col(\"AreaAct\"))\n",
    ")\n",
    "\n",
    "df_geografia = df_geografia.withColumn(\n",
    "    \"DensidadAct\",\n",
    "    when(col(\"DensidadAct\").isNull(), col(\"PromedioDensidadEstado\")).otherwise(col(\"DensidadAct\"))\n",
    ")\n",
    "\n",
    "# Eliminar las columnas de promedios temporales\n",
    "df_geografia = df_geografia.drop(\"PromedioAreaEstado\", \"PromedioDensidadEstado\")\n",
    "\n",
    "# Validar los cambios\n",
    "df_geografia.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhFsv4ba86vw"
   },
   "source": [
    "## Dimensión Areas de Servicio\n",
    "\n",
    "\n",
    "AJUSTAR EL BLOQUE DEL DISENO PARA EL NUEVO MODELO DADO EN CLASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validar la Tabla Rs_AreasDeServicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa con la tabla Rs_AreasDeServicio:\n",
      "+--------------------+------------------+---------------------------------------------+------------+\n",
      "|IdAreaDeServicio_DWH|IdAreaDeServicio_T|Nombre                                       |AnnoCreacion|\n",
      "+--------------------+------------------+---------------------------------------------+------------+\n",
      "|1                   |100622017         |New Jersey - Medical91661NJ2340003-0520174859|2017        |\n",
      "|2                   |100722019         |New Jersey - Medical91661NJ2340003-0520194597|2019        |\n",
      "|3                   |100922020         |New Jersey - Medical91661NJ2340003-0520204858|2020        |\n",
      "|4                   |101012018         |New Jersey - Medical91661NJ2340003-0520184611|2018        |\n",
      "|5                   |101062020         |New Jersey - Medical91661NJ2340003-0520205037|2020        |\n",
      "+--------------------+------------------+---------------------------------------------+------------+\n",
      "\n",
      "Columnas de la tabla Rs_AreasDeServicio:\n",
      "+--------------------+\n",
      "|COLUMN_NAME         |\n",
      "+--------------------+\n",
      "|AnnoCreacion        |\n",
      "|IdAreaDeServicio_DWH|\n",
      "|IdAreaDeServicio_T  |\n",
      "|Nombre              |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar si la conexión con la tabla `Rs_AreasDeServicio` funciona correctamente\n",
    "sql_areas_servicio_test = '''\n",
    "(SELECT * \n",
    " FROM Estudiante_8_202415.Rs_AreasDeServicio\n",
    " LIMIT 5) AS Rs_AreasDeServicio_Test\n",
    "'''\n",
    "\n",
    "try:\n",
    "    areas_servicio_test = conn_orig.get_dataframe(sql_areas_servicio_test)\n",
    "    print(\"Conexión exitosa con la tabla Rs_AreasDeServicio:\")\n",
    "    areas_servicio_test.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar con la tabla Rs_AreasDeServicio:\")\n",
    "    print(e)\n",
    "\n",
    "sql_describe_table = '''\n",
    "(SELECT COLUMN_NAME \n",
    " FROM INFORMATION_SCHEMA.COLUMNS \n",
    " WHERE TABLE_SCHEMA = 'Estudiante_8_202415'\n",
    "   AND TABLE_NAME = 'Rs_AreasDeServicio') AS ColumnsInfo\n",
    "'''\n",
    "\n",
    "try:\n",
    "    columns_info = conn_orig.get_dataframe(sql_describe_table)\n",
    "    print(\"Columnas de la tabla Rs_AreasDeServicio:\")\n",
    "    columns_info.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al listar columnas de la tabla Rs_AreasDeServicio:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Validar la Tabla Rs_AsociacionAreaServicioGeografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa con la tabla Rs_AsociacionAreaServicioGeografia:\n",
      "+--------------------+---------------+\n",
      "|IdAreaDeServicio_DWH|IdGeografia_DWH|\n",
      "+--------------------+---------------+\n",
      "+--------------------+---------------+\n",
      "\n",
      "Columnas de la tabla Rs_AsociacionAreaServicioGeografia:\n",
      "+--------------------+\n",
      "|COLUMN_NAME         |\n",
      "+--------------------+\n",
      "|IdAreaDeServicio_DWH|\n",
      "|IdGeografia_DWH     |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar la conexión con la tabla Rs_AsociacionAreaServicioGeografia\n",
    "sql_asociacion_test = '''\n",
    "(SELECT * \n",
    " FROM Estudiante_8_202415.Rs_AsociacionAreaServicioGeografia\n",
    " LIMIT 5) AS Rs_AsociacionAreaServicioGeografia_Test\n",
    "'''\n",
    "\n",
    "try:\n",
    "    asociacion_test = conn_orig.get_dataframe(sql_asociacion_test)\n",
    "    print(\"Conexión exitosa con la tabla Rs_AsociacionAreaServicioGeografia:\")\n",
    "    asociacion_test.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar con la tabla Rs_AsociacionAreaServicioGeografia:\")\n",
    "    print(e)\n",
    "\n",
    "sql_describe_table = '''\n",
    "(SELECT COLUMN_NAME \n",
    " FROM INFORMATION_SCHEMA.COLUMNS \n",
    " WHERE TABLE_SCHEMA = 'Estudiante_8_202415'\n",
    "   AND TABLE_NAME = 'Rs_AsociacionAreaServicioGeografia') AS ColumnsInfo\n",
    "'''\n",
    "\n",
    "try:\n",
    "    columns_info = conn_orig.get_dataframe(sql_describe_table)\n",
    "    print(\"Columnas de la tabla Rs_AsociacionAreaServicioGeografia:\")\n",
    "    columns_info.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al listar columnas de la tabla Rs_AsociacionAreaServicioGeografia:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Validar la Tabla Rs_AsociacionAreaServicioGeografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa con la tabla Rs_Geografia:\n",
      "+---------------+-------------+------+-------+-------+-----------+------------+\n",
      "|IdGeografia_DWH|IdGeografia_T|Estado|Condado|AreaAct|DensidadAct|PoblacionAct|\n",
      "+---------------+-------------+------+-------+-------+-----------+------------+\n",
      "+---------------+-------------+------+-------+-------+-----------+------------+\n",
      "\n",
      "Columnas de la tabla Rs_Geografia:\n",
      "+---------------+\n",
      "|COLUMN_NAME    |\n",
      "+---------------+\n",
      "|AreaAct        |\n",
      "|Condado        |\n",
      "|DensidadAct    |\n",
      "|Estado         |\n",
      "|IdGeografia_DWH|\n",
      "|IdGeografia_T  |\n",
      "|PoblacionAct   |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar la conexión con la tabla Rs_Geografia\n",
    "sql_geografia_test = '''\n",
    "(SELECT * \n",
    " FROM Estudiante_8_202415.Rs_Geografia\n",
    " LIMIT 5) AS Rs_Geografia_Test\n",
    "'''\n",
    "\n",
    "try:\n",
    "    geografia_test = conn_orig.get_dataframe(sql_geografia_test)\n",
    "    print(\"Conexión exitosa con la tabla Rs_Geografia:\")\n",
    "    geografia_test.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al conectar con la tabla Rs_Geografia:\")\n",
    "    print(e)\n",
    "\n",
    "# Alternativa: Obtener información sobre las columnas de Rs_Geografia\n",
    "sql_describe_geografia = '''\n",
    "(SELECT COLUMN_NAME \n",
    " FROM INFORMATION_SCHEMA.COLUMNS \n",
    " WHERE TABLE_SCHEMA = 'Estudiante_8_202415'\n",
    "   AND TABLE_NAME = 'Rs_Geografia') AS ColumnsInfo\n",
    "'''\n",
    "\n",
    "try:\n",
    "    columns_info_geografia = conn_orig.get_dataframe(sql_describe_geografia)\n",
    "    print(\"Columnas de la tabla Rs_Geografia:\")\n",
    "    columns_info_geografia.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error al listar columnas de la tabla Rs_Geografia:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# AsociacionAreaServicioGeografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de datos de AsociacionAreaServicioGeografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 registros de mapeo IdAreaDeServicio ordenados:\n",
      "+--------------------+------------------+\n",
      "|IdAreaDeServicio_DWH|IdAreaDeServicio_T|\n",
      "+--------------------+------------------+\n",
      "|1                   |100622017         |\n",
      "|2                   |100722019         |\n",
      "|3                   |100922020         |\n",
      "|4                   |101012018         |\n",
      "|5                   |101062020         |\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracción desde Rs_AreasDeServicio\n",
    "sql_cargar_areas_servicio = '''\n",
    "(SELECT \n",
    "    IdAreaDeServicio_DWH,\n",
    "    IdAreaDeServicio_T\n",
    " FROM Estudiante_8_202415.Rs_AreasDeServicio) AS AreasDeServicioMapeo\n",
    "'''\n",
    "df_areas_servicio_mapeo = conn_dest.get_dataframe(sql_cargar_areas_servicio)\n",
    "\n",
    "# Ordenar por IdAreaDeServicio_DWH de menor a mayor\n",
    "df_areas_servicio_mapeo_sorted = df_areas_servicio_mapeo.orderBy(\"IdAreaDeServicio_DWH\", ascending=True)\n",
    "\n",
    "print(\"Primeros 5 registros de mapeo IdAreaDeServicio ordenados:\")\n",
    "df_areas_servicio_mapeo_sorted.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Validar valores null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Revisar tipo de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IdAreaDeServicio_DWH: integer (nullable = false)\n",
      " |-- IdGeografia_DWH: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el esquema\n",
    "df_asociacion_area_servicio_geografia.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vtE61qk986vx",
    "X9Oou0g986vy",
    "B5kFkHTD86vz",
    "ddFhEOmL86vz",
    "MuvVgJ4R86v0",
    "BZjDeVYd86v1",
    "_7xbgfCk86v1",
    "Rh_102Yy86v1",
    "HogvCqW_86v2",
    "9rKmT9jd86v2",
    "R9SnkMUH86v3",
    "k8O1GvOd86v3",
    "Mo633Vpg86v3",
    "i36NawhX86v4",
    "LvgnsPfK86v4",
    "dTv_CIOT86v5",
    "N9f10qpB86v9"
   ],
   "name": "MISW-ETL-TutorialETL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

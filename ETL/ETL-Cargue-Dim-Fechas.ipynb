{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Configuración Inicial\n",
    "Métodos un utils en común en los notebooks"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:06:32.138435Z",
     "start_time": "2024-11-21T03:06:32.127034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports \n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from pyspark.sql import functions as f, SparkSession\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import col\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:00:38.341895Z",
     "start_time": "2024-11-21T03:00:38.336125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySQLConnector:\n",
    "    def __init__(self, spark: SparkSession, connection_properties: dict, url: str):\n",
    "        self.spark = spark\n",
    "        self.properties = connection_properties\n",
    "        self.url = url\n",
    "\n",
    "    def get_dataframe(self, sql_query: str):        \n",
    "        df = self.spark.read.jdbc(\n",
    "            url=self.url,\n",
    "            table=sql_query,\n",
    "            properties=self.properties\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def save_db(self, df, tabla):\n",
    "        df.write.jdbc(\n",
    "            url=self.url,\n",
    "            table=tabla,\n",
    "            mode='append',\n",
    "            properties=self.properties\n",
    "        )\n",
    "        \n",
    "def create_spark_session(path_jar_driver):    \n",
    "    conf = SparkConf().set('spark.driver.extraClassPath', path_jar_driver)\n",
    "    spark_context = SparkContext(conf=conf)\n",
    "    sql_context = SQLContext(spark_context)\n",
    "    return sql_context.sparkSession    \n",
    "\n",
    "def get_dataframe_from_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "os1iJYmu86vt",
    "ExecuteTime": {
     "end_time": "2024-11-21T03:00:51.889362Z",
     "start_time": "2024-11-21T03:00:51.886495Z"
    }
   },
   "source": [
    "db_user = 'Estudiante_65_202415'\n",
    "db_psswd = 'Estudiante_202010409'\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_psswd,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source_db_string_connection = 'jdbc:mysql://157.253.236.120:8080/RaSaTransaccional_ETL'\n",
    "destination_db_string_connection = f'jdbc:mysql://157.253.236.120:8080/{db_user}'\n",
    "\n",
    "# Driver de conexion\n",
    "# LINUX\n",
    "path_jar_driver = '/opt/mysql/lib/mysql-connector-java-8.0.28.jar'\n",
    "# WINDOWS como esta en la VM \n",
    "#path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:01:01.907797Z",
     "start_time": "2024-11-21T03:00:57.596586Z"
    }
   },
   "cell_type": "code",
   "source": "spark = create_spark_session(path_jar_driver)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 22:00:59 WARN Utils: Your hostname, willp resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface enp8s0)\n",
      "24/11/20 22:00:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/20 22:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/20 22:01:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/willp/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:01:04.652771Z",
     "start_time": "2024-11-21T03:01:04.650087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conn_orig = MySQLConnector(spark=spark, connection_properties=connection_properties, url=source_db_string_connection)\n",
    "conn_dest = MySQLConnector(spark=spark, connection_properties=connection_properties, url=destination_db_string_connection)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "LqQVz5s686vq"
   },
   "cell_type": "markdown",
   "source": "# Dimensión Fecha"
  },
  {
   "metadata": {
    "id": "A2UT2Ia586vr"
   },
   "cell_type": "markdown",
   "source": "Fecha es importante para llevar la historia de las tablas hechos."
  },
  {
   "metadata": {
    "id": "UpOSEfhX86vs"
   },
   "cell_type": "markdown",
   "source": "![Modelo Movimientos](./images/Fecha.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extraction"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se hace un formateo de las fechas usando SQL para estandarizar la fecha."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:01:15.885954Z",
     "start_time": "2024-11-21T03:01:10.472745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACCION\n",
    "sql_move_date = '''\n",
    "(\n",
    "SELECT DISTINCT\n",
    "    CASE\n",
    "        WHEN Fecha REGEXP '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\n",
    "            THEN DATE_FORMAT(STR_TO_DATE(TRIM(Fecha), '%Y-%m-%d %H:%i:%s.%f'), '%Y-%m-%d')\n",
    "        WHEN Fecha REGEXP '^[A-Za-z]{3} [0-9]{1,2},[0-9]{4}$'\n",
    "            THEN DATE_FORMAT(STR_TO_DATE(TRIM(Fecha), '%b %d,%Y'), '%Y-%m-%d')\n",
    "        ELSE concat('Invalid Format: ',Fecha)\n",
    "        END AS Fecha\n",
    "FROM FuentePlanesBeneficio_ETL\n",
    ") AS Fecha\n",
    "'''\n",
    "df_move_date = conn_orig.get_dataframe(sql_move_date)\n",
    "df_supplier_move_date = df_move_date.withColumn('Fecha', col('Fecha').cast(DateType()))\n",
    "df_supplier_move_date.show(5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     Fecha|\n",
      "+----------+\n",
      "|2017-12-31|\n",
      "|2019-12-31|\n",
      "|2020-12-31|\n",
      "|2021-12-31|\n",
      "|2018-12-31|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformation\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adicionar las columnas de la dimension"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:01:19.988718Z",
     "start_time": "2024-11-21T03:01:19.911910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_supplier_move_date = df_supplier_move_date.withColumn(\n",
    "    \"IdFecha\", f.date_format(\"Fecha\", \"yyyyMMdd\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Dia\", f.dayofmonth(\"Fecha\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Mes\", f.month(\"Fecha\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Annio\", f.year(\"Fecha\").cast(\"int\"))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:01:56.308351Z",
     "start_time": "2024-11-21T03:01:56.303922Z"
    }
   },
   "cell_type": "code",
   "source": "df_supplier_move_date.printSchema()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fecha: date (nullable = true)\n",
      " |-- IdFecha: integer (nullable = true)\n",
      " |-- Dia: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Annio: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adicionar comodín\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:08:09.328839Z",
     "start_time": "2024-11-21T03:08:08.475032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dates_data = [( \"9999-12-31\", 99991231, 31, 12, 9999)]\n",
    "dates_columns = [\"Fecha\", \"IdFecha\", \"Dia\", \"Mes\", \"Annio\"]\n",
    "dummy_dates = spark.createDataFrame(dates_data, dates_columns)\n",
    "\n",
    "dummy_dates = dummy_dates.withColumn('IdFecha', col('IdFecha').cast(IntegerType())) \\\n",
    "    .withColumn('Fecha', col('Fecha').cast(DateType())) \\\n",
    "    .withColumn('Dia', col('Dia').cast(IntegerType())) \\\n",
    "    .withColumn('Mes', col('Mes').cast(IntegerType())) \\\n",
    "    .withColumn('Annio', col('Annio').cast(IntegerType()))\n",
    "dummy_dates.show()\n",
    "dummy_dates.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---+-----+\n",
      "|     Fecha| IdFecha|Dia|Mes|Annio|\n",
      "+----------+--------+---+---+-----+\n",
      "|9999-12-31|99991231| 31| 12| 9999|\n",
      "+----------+--------+---+---+-----+\n",
      "\n",
      "root\n",
      " |-- Fecha: date (nullable = true)\n",
      " |-- IdFecha: integer (nullable = true)\n",
      " |-- Dia: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Annio: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:08:13.994649Z",
     "start_time": "2024-11-21T03:08:12.049798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_supplier_move_date = dummy_dates.union(df_supplier_move_date)\n",
    "df_supplier_move_date.show(40)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---+-----+\n",
      "|     Fecha| IdFecha|Dia|Mes|Annio|\n",
      "+----------+--------+---+---+-----+\n",
      "|9999-12-31|99991231| 31| 12| 9999|\n",
      "|2017-12-31|20171231| 31| 12| 2017|\n",
      "|2019-12-31|20191231| 31| 12| 2019|\n",
      "|2020-12-31|20201231| 31| 12| 2020|\n",
      "|2021-12-31|20211231| 31| 12| 2021|\n",
      "|2018-12-31|20181231| 31| 12| 2018|\n",
      "+----------+--------+---+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:08:25.909680Z",
     "start_time": "2024-11-21T03:08:25.832194Z"
    }
   },
   "cell_type": "code",
   "source": "df_supplier_move_date.orderBy(col('IdFecha'))",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Fecha: date, IdFecha: int, Dia: int, Mes: int, Annio: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Persistencia de la dimension."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:08:30.048557Z",
     "start_time": "2024-11-21T03:08:28.333902Z"
    }
   },
   "cell_type": "code",
   "source": "conn_dest.save_db(df_supplier_move_date, \"Rs_Fecha\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vtE61qk986vx",
    "X9Oou0g986vy",
    "B5kFkHTD86vz",
    "ddFhEOmL86vz",
    "MuvVgJ4R86v0",
    "BZjDeVYd86v1",
    "_7xbgfCk86v1",
    "Rh_102Yy86v1",
    "HogvCqW_86v2",
    "9rKmT9jd86v2",
    "R9SnkMUH86v3",
    "k8O1GvOd86v3",
    "Mo633Vpg86v3",
    "i36NawhX86v4",
    "LvgnsPfK86v4",
    "dTv_CIOT86v5",
    "N9f10qpB86v9"
   ],
   "name": "MISW-ETL-TutorialETL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

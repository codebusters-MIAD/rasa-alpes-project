{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcY9y56n86vn"
   },
   "source": "ETL: Carga Areas de Servicio"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:38:32.336937Z",
     "start_time": "2024-11-16T16:38:32.266872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports \n",
    "from pyspark.sql.types import IntegerType, StringType, DateType, LongType\n",
    "from pyspark.sql import functions as f, SparkSession, types as t\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import col, length\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqQVz5s686vq"
   },
   "source": "## Proceso de ETL para una dimensión."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2UT2Ia586vr"
   },
   "source": "Fecha es importante para llevar el historico de las tablas hechos."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpOSEfhX86vs"
   },
   "source": "![Modelo Movimientos](./images/Fecha.png)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:38:35.158058Z",
     "start_time": "2024-11-16T16:38:35.153175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySQLConnector:\n",
    "    def __init__(self, spark: SparkSession, connection_properties: dict, url: str):\n",
    "        self.spark = spark\n",
    "        self.properties = connection_properties\n",
    "        self.url = url\n",
    "\n",
    "    def get_dataframe(self, sql_query: str):        \n",
    "        df = self.spark.read.jdbc(\n",
    "            url=self.url,\n",
    "            table=sql_query,\n",
    "            properties=self.properties\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def save_db(self, df, tabla):\n",
    "        df.write.jdbc(\n",
    "            url=self.url,\n",
    "            table=tabla,\n",
    "            mode='append',\n",
    "            properties=self.properties\n",
    "        )\n",
    "        \n",
    "def create_spark_session(path_jar_driver):    \n",
    "    conf = SparkConf().set('spark.driver.extraClassPath', path_jar_driver)\n",
    "    spark_context = SparkContext(conf=conf)\n",
    "    sql_context = SQLContext(spark_context)\n",
    "    return sql_context.sparkSession    \n",
    "\n",
    "def get_dataframe_from_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "os1iJYmu86vt",
    "ExecuteTime": {
     "end_time": "2024-11-16T16:38:38.287079Z",
     "start_time": "2024-11-16T16:38:38.282905Z"
    }
   },
   "source": [
    "# LLENAR CON EL USUARIO DE CADA UNO\n",
    "db_user = 'Estudiante_65_202415'\n",
    "db_psswd = 'Estudiante_202010409'\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_psswd,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source_db_string_connection = 'jdbc:mysql://157.253.236.120:8080/RaSaTransaccional_ETL'\n",
    "destination_db_string_connection = f'jdbc:mysql://157.253.236.120:8080/{db_user}'\n",
    "\n",
    "# Driver de conexion\n",
    "# LINUX\n",
    "path_jar_driver = '/opt/mysql/lib/mysql-connector-java-8.0.28.jar'\n",
    "# WINDOWS\n",
    "#path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:38:44.797613Z",
     "start_time": "2024-11-16T16:38:41.348550Z"
    }
   },
   "cell_type": "code",
   "source": "spark = create_spark_session(path_jar_driver)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/16 11:38:43 WARN Utils: Your hostname, willp resolves to a loopback address: 127.0.1.1; using 192.168.1.113 instead (on interface wlp9s0)\n",
      "24/11/16 11:38:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/16 11:38:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/16 11:38:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/willp/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:38:47.872467Z",
     "start_time": "2024-11-16T16:38:47.867225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conn_orig = MySQLConnector(spark=spark, connection_properties=connection_properties, url=source_db_string_connection)\n",
    "conn_dest = MySQLConnector(spark=spark, connection_properties=connection_properties, url=destination_db_string_connection)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhFsv4ba86vw"
   },
   "source": [
    "## Dimensión Fecha\n",
    "\n",
    "\n",
    "AJUSTAR EL BLOQUE DEL DISENO PARA EL NUEVO MODELO DADO EN CLASE\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extraction"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:58:58.260335Z",
     "start_time": "2024-11-16T16:58:57.708504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACCION\n",
    "sql_move_date = '''\n",
    "(\n",
    "SELECT DISTINCT\n",
    "    CASE\n",
    "        WHEN Fecha REGEXP '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\n",
    "            THEN DATE_FORMAT(STR_TO_DATE(TRIM(Fecha), '%Y-%m-%d %H:%i:%s.%f'), '%Y-%m-%d')\n",
    "        WHEN Fecha REGEXP '^[A-Za-z]{3} [0-9]{1,2},[0-9]{4}$'\n",
    "            THEN DATE_FORMAT(STR_TO_DATE(TRIM(Fecha), '%b %d,%Y'), '%Y-%m-%d')\n",
    "        ELSE concat('Invalid Format: ',Fecha)\n",
    "        END AS Fecha\n",
    "FROM FuentePlanesBeneficio_ETL\n",
    ") AS Fecha\n",
    "'''\n",
    "df_move_date = conn_orig.get_dataframe(sql_move_date)\n",
    "df_supplier_move_date = df_move_date.withColumn('Fecha', col('Fecha').cast(DateType()))\n",
    "df_supplier_move_date.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     Fecha|\n",
      "+----------+\n",
      "|2017-12-31|\n",
      "|2019-12-31|\n",
      "|2020-12-31|\n",
      "|2021-12-31|\n",
      "|2018-12-31|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformation\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:59:00.198077Z",
     "start_time": "2024-11-16T16:59:00.142663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_supplier_move_date = df_supplier_move_date.withColumn(\n",
    "    \"IdFecha\", f.date_format(\"Fecha\", \"yyyyMMdd\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Dia\", f.dayofmonth(\"Fecha\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Mes\", f.month(\"Fecha\").cast(\"int\")\n",
    ").withColumn(\n",
    "    \"Annio\", f.year(\"Fecha\").cast(\"int\"))"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T16:59:04.168499Z",
     "start_time": "2024-11-16T16:59:02.674846Z"
    }
   },
   "cell_type": "code",
   "source": "conn_dest.save_db(df_supplier_move_date, \"Rs_Fecha\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vtE61qk986vx",
    "X9Oou0g986vy",
    "B5kFkHTD86vz",
    "ddFhEOmL86vz",
    "MuvVgJ4R86v0",
    "BZjDeVYd86v1",
    "_7xbgfCk86v1",
    "Rh_102Yy86v1",
    "HogvCqW_86v2",
    "9rKmT9jd86v2",
    "R9SnkMUH86v3",
    "k8O1GvOd86v3",
    "Mo633Vpg86v3",
    "i36NawhX86v4",
    "LvgnsPfK86v4",
    "dTv_CIOT86v5",
    "N9f10qpB86v9"
   ],
   "name": "MISW-ETL-TutorialETL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
